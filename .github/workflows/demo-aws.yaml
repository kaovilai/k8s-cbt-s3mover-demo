name: K8s CBT Demo on AWS EKS

# This workflow creates a temporary EKS cluster for testing CBT with real block devices
# Uses CSI hostpath driver with CBT support (AWS EBS CSI driver doesn't support CBT yet)
# Requires AWS credentials as GitHub secrets:
# - AWS_ACCESS_KEY_ID
# - AWS_SECRET_ACCESS_KEY
# - AWS_REGION (optional, defaults to us-east-1)

on:
  workflow_dispatch:
    inputs:
      cluster_name:
        description: 'EKS cluster name (will be prefixed with cbt-demo-)'
        required: false
        default: 'test'
      keep_cluster:
        description: 'Keep cluster after tests (true/false)'
        required: false
        default: 'false'

env:
  AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}
  CLUSTER_NAME: cbt-demo-${{ github.event.inputs.cluster_name || 'test' }}-${{ github.run_number }}
  EKS_VERSION: '1.34'  # CBT alpha support added in k8s 1.33, using 1.34 (latest)

jobs:
  test-on-eks:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install eksctl
        run: |
          ARCH=amd64
          PLATFORM=$(uname -s)_$ARCH
          curl -sLO "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
          tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin
          eksctl version

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          kubectl version --client

      - name: Create EKS cluster
        run: |
          echo "Creating EKS cluster: $CLUSTER_NAME in region: $AWS_REGION"

          cat > eks-cluster-config.yaml <<EOF
          apiVersion: eksctl.io/v1alpha5
          kind: ClusterConfig

          metadata:
            name: $CLUSTER_NAME
            region: $AWS_REGION
            version: "$EKS_VERSION"
            tags:
              purpose: cbt-demo
              ci: github-actions
              auto-delete: "true"

          managedNodeGroups:
            - name: ng-1
              instanceType: t3.medium
              desiredCapacity: 2
              minSize: 2
              maxSize: 2
              volumeSize: 20
              volumeType: gp3
              tags:
                purpose: cbt-demo
              iam:
                withAddonPolicies:
                  ebs: true
                  efs: true

          addons:
            - name: aws-ebs-csi-driver
              version: latest
              attachPolicyARNs:
                - arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy
          EOF

          eksctl create cluster -f eks-cluster-config.yaml

          # Update kubeconfig
          aws eks update-kubeconfig --name $CLUSTER_NAME --region $AWS_REGION

      - name: Verify EKS cluster
        run: |
          echo "Waiting for cluster to be fully ready..."
          kubectl wait --for=condition=Ready nodes --all --timeout=300s

          echo "Cluster nodes:"
          kubectl get nodes -o wide

          echo ""
          echo "Kubernetes version:"
          kubectl version --output=yaml | grep gitVersion

          echo ""
          echo "Storage classes:"
          kubectl get storageclass

          echo ""
          echo "Note: CBT alpha APIs are available by default in k8s 1.33+"
          echo "No feature gates required - enabled through CSI driver implementation"

      - name: Install VolumeSnapshot CRDs
        run: |
          echo "Installing VolumeSnapshot CRDs..."
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/client/config/crd/groupsnapshot.storage.k8s.io_volumegroupsnapshotclasses.yaml
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/client/config/crd/groupsnapshot.storage.k8s.io_volumegroupsnapshotcontents.yaml
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/client/config/crd/groupsnapshot.storage.k8s.io_volumegroupsnapshots.yaml

      - name: Deploy CSI Driver with CBT
        run: |
          echo "Deploying CSI hostpath driver with Changed Block Tracking..."
          ./scripts/02-deploy-csi-driver.sh

          # Verify CBT CRD installation
          echo ""
          echo "Verifying SnapshotMetadataService CRD installation..."
          if kubectl get crd snapshotmetadataservices.cbt.storage.k8s.io &> /dev/null; then
            echo "✓ SnapshotMetadataService CRD is properly installed"
            kubectl get crd snapshotmetadataservices.cbt.storage.k8s.io
          else
            echo "⚠️ Warning: SnapshotMetadataService CRD not found"
            echo "CBT functionality may be limited"
            echo "This is expected as the CRD is still in alpha and may not be publicly available yet"
          fi

      - name: Validate CBT Setup
        run: |
          echo "Validating CBT configuration..."
          ./scripts/validate-cbt.sh

      - name: Deploy MinIO
        run: |
          echo "Deploying MinIO S3 storage..."
          ./scripts/01-deploy-minio.sh

          # Wait for MinIO
          kubectl wait --for=condition=Ready pod -l app=minio -n cbt-demo --timeout=300s
          kubectl get pods -n cbt-demo -l app=minio

      - name: Deploy PostgreSQL Workload
        run: |
          echo "Deploying PostgreSQL workload with block PVC..."
          ./scripts/03-deploy-workload.sh

      - name: Verify PostgreSQL deployment
        run: |
          echo "Waiting for PostgreSQL pod..."
          kubectl wait --for=condition=Ready pod -l app=postgres -n cbt-demo --timeout=300s || {
            echo "PostgreSQL not ready, checking status..."
            kubectl get pods -n cbt-demo -l app=postgres
            kubectl describe pod -n cbt-demo -l app=postgres
            kubectl get pvc -n cbt-demo
            kubectl describe pvc -n cbt-demo
          }

      - name: Run integrity checks
        run: |
          echo "Running integrity checks..."
          ./scripts/backup-status.sh || true
          ./scripts/integrity-check.sh || true

      - name: Create and test snapshots
        run: |
          echo "Creating first VolumeSnapshot..."

          PVC_NAME=$(kubectl get pvc -n cbt-demo -l app=postgres -o jsonpath='{.items[0].metadata.name}')
          echo "PVC Name: $PVC_NAME"

          cat <<EOF | kubectl apply -f -
          apiVersion: snapshot.storage.k8s.io/v1
          kind: VolumeSnapshot
          metadata:
            name: postgres-snapshot-1
            namespace: cbt-demo
          spec:
            volumeSnapshotClassName: csi-hostpath-snapclass
            source:
              persistentVolumeClaimName: $PVC_NAME
          EOF

          # Wait for snapshot to be ready (using kubectl wait with jsonpath - upstream best practice)
          echo "Waiting for snapshot 1 to be ready..."
          kubectl wait volumesnapshot postgres-snapshot-1 -n cbt-demo \
            --for=jsonpath='{.status.readyToUse}'=true --timeout=300s || {
            echo "✗ Snapshot did not become ready within timeout"
            kubectl get volumesnapshot postgres-snapshot-1 -n cbt-demo
            kubectl describe volumesnapshot postgres-snapshot-1 -n cbt-demo
            exit 1
          }
          echo "✓ Snapshot 1 is ready!"
          kubectl get volumesnapshot postgres-snapshot-1 -n cbt-demo -o yaml

          echo ""
          echo "Writing additional data to PostgreSQL to create changed blocks..."
          kubectl exec -n cbt-demo -it postgres-0 -- psql -U demo -d cbtdemo -c \
            "INSERT INTO demo_data (data_block, content, checksum)
             SELECT generate_series(101, 200),
                    encode(gen_random_bytes(100000), 'base64'),
                    md5(random()::text);" || echo "Note: Data insert may fail in non-block environments"

          echo "Data written. Now creating second VolumeSnapshot for delta testing..."

          cat <<EOF | kubectl apply -f -
          apiVersion: snapshot.storage.k8s.io/v1
          kind: VolumeSnapshot
          metadata:
            name: postgres-snapshot-2
            namespace: cbt-demo
          spec:
            volumeSnapshotClassName: csi-hostpath-snapclass
            source:
              persistentVolumeClaimName: $PVC_NAME
          EOF

          # Wait for snapshot 2 to be ready
          echo "Waiting for snapshot 2 to be ready..."
          kubectl wait volumesnapshot postgres-snapshot-2 -n cbt-demo \
            --for=jsonpath='{.status.readyToUse}'=true --timeout=300s || {
            echo "✗ Snapshot did not become ready within timeout"
            kubectl get volumesnapshot postgres-snapshot-2 -n cbt-demo
            kubectl describe volumesnapshot postgres-snapshot-2 -n cbt-demo
            exit 1
          }
          echo "✓ Snapshot 2 is ready!"

          echo ""
          echo "Testing CBT API with both snapshot name and CSI handle..."
          echo ""
          echo "Note: PR kubernetes-csi/external-snapshot-metadata#180 added support for using"
          echo "CSI snapshot handles in addition to snapshot names. This demonstrates both methods."
          echo ""

          # Get CSI snapshot handle from VolumeSnapshotContent
          VSC_NAME=$(kubectl get volumesnapshot postgres-snapshot-1 -n cbt-demo -o jsonpath="{.status.boundVolumeSnapshotContentName}")
          echo "VolumeSnapshotContent for VolumeSnapshot postgres-snapshot-1 is [$VSC_NAME]"

          SNAP_HANDLE=$(kubectl get volumesnapshotcontent $VSC_NAME -o jsonpath="{.status.snapshotHandle}")
          echo "CSI snapshot handle of VolumeSnapshot postgres-snapshot-1 is [$SNAP_HANDLE]"

          echo ""
          echo "The snapshots now capture different states:"
          echo "  - Snapshot 1: Initial data (100 rows)"
          echo "  - Snapshot 2: After inserting 100 additional rows (~10MB of changes)"
          echo ""
          echo "When the snapshot metadata lister/verifier tools become available, they can be used with:"
          echo "  # Using snapshot name (traditional):"
          echo "  snapshot-metadata-lister -p postgres-snapshot-1 -s postgres-snapshot-2 -n cbt-demo"
          echo ""
          echo "  # Using CSI handle (new approach from PR #180, preferred):"
          echo "  snapshot-metadata-lister -P '$SNAP_HANDLE' -s postgres-snapshot-2 -n cbt-demo"
          echo ""
          echo "The CSI handle approach is preferred as it allows the base snapshot to be deleted"
          echo "while still being able to compute deltas using its CSI handle."
          echo ""
          echo "The GetMetadataDelta API should report the changed blocks between the two snapshots."

      - name: Test disaster recovery
        run: |
          echo "Testing disaster recovery scenario..."
          ./scripts/05-simulate-disaster.sh <<< "yes" || true

      - name: Collect logs on failure
        if: failure()
        run: |
          echo "========================================"
          echo "Collecting diagnostic information..."
          echo "========================================"

          echo ""
          echo "All resources across namespaces:"
          kubectl get all -A

          echo ""
          echo "Detailed pod information:"
          kubectl get po -A --show-labels

          echo ""
          echo "VolumeSnapshots:"
          kubectl get volumesnapshot -A

          echo ""
          echo "VolumeSnapshotContents:"
          kubectl get volumesnapshotcontent

          echo ""
          echo "PVCs:"
          kubectl get pvc -A

          echo ""
          echo "PVs:"
          kubectl get pv

          echo ""
          echo "StorageClasses:"
          kubectl get storageclass

          echo ""
          echo "VolumeSnapshotClasses:"
          kubectl get volumesnapshotclass

          echo ""
          echo "SnapshotMetadataServices:"
          kubectl get snapshotmetadataservices -A || echo "CRD not installed"

          echo ""
          echo "========================================"
          echo "MinIO Logs:"
          echo "========================================"
          kubectl logs -n cbt-demo -l app=minio --tail=100 || true
          kubectl describe pod -n cbt-demo -l app=minio || true

          echo ""
          echo "========================================"
          echo "PostgreSQL Logs:"
          echo "========================================"
          kubectl logs -n cbt-demo -l app=postgres --tail=100 || true
          kubectl describe pod -n cbt-demo -l app=postgres || true

          echo ""
          echo "========================================"
          echo "CSI Driver Logs:"
          echo "========================================"
          kubectl logs -n default csi-hostpathplugin-0 --all-containers --tail=100 || true
          kubectl describe pod -n default csi-hostpathplugin-0 || true

      - name: Cleanup namespace
        if: always()
        run: |
          echo "Cleaning up cbt-demo namespace..."
          kubectl delete namespace cbt-demo --ignore-not-found=true --timeout=120s || true

      - name: Delete EKS cluster
        if: always() && github.event.inputs.keep_cluster != 'true'
        run: |
          echo "Deleting EKS cluster: $CLUSTER_NAME"
          eksctl delete cluster --name $CLUSTER_NAME --region $AWS_REGION --wait || {
            echo "Standard deletion failed, forcing cleanup..."
            eksctl delete cluster --name $CLUSTER_NAME --region $AWS_REGION --force || true
          }

      - name: Verify cleanup
        if: always() && github.event.inputs.keep_cluster != 'true'
        run: |
          echo "Verifying cluster deletion..."
          aws eks describe-cluster --name $CLUSTER_NAME --region $AWS_REGION && {
            echo "⚠️  Cluster still exists!"
            exit 1
          } || echo "✓ Cluster successfully deleted"

      - name: Export cluster info (if kept)
        if: always() && github.event.inputs.keep_cluster == 'true'
        run: |
          echo "=========================================="
          echo "Cluster was kept for manual testing"
          echo "=========================================="
          echo ""
          echo "Cluster name: $CLUSTER_NAME"
          echo "Region: $AWS_REGION"
          echo ""
          echo "To access:"
          echo "  aws eks update-kubeconfig --name $CLUSTER_NAME --region $AWS_REGION"
          echo ""
          echo "To delete when done:"
          echo "  eksctl delete cluster --name $CLUSTER_NAME --region $AWS_REGION"
          echo ""
          echo "⚠️  IMPORTANT: Remember to delete this cluster to avoid charges!"
          echo "=========================================="

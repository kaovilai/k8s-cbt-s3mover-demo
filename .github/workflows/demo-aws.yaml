name: K8s CBT Demo on AWS EKS

# This workflow creates a temporary EKS cluster for testing CBT with real block devices
# Uses CSI hostpath driver with CBT support (AWS EBS CSI driver doesn't support CBT yet)
# Requires AWS credentials as GitHub secrets:
# - AWS_ACCESS_KEY_ID
# - AWS_SECRET_ACCESS_KEY
# - AWS_REGION (optional, defaults to us-east-1)

on:
  workflow_dispatch:
    inputs:
      cluster_name:
        description: 'EKS cluster name (will be prefixed with cbt-demo-)'
        required: false
        default: 'test'
      keep_cluster:
        description: 'Keep cluster after tests (true/false)'
        required: false
        default: 'false'

env:
  AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}
  CLUSTER_NAME: cbt-demo-${{ github.event.inputs.cluster_name || 'test' }}-${{ github.run_number }}
  EKS_VERSION: '1.34'  # CBT alpha support added in k8s 1.33, using 1.34 (latest)

jobs:
  test-on-eks:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install eksctl
        run: |
          ARCH=amd64
          PLATFORM=$(uname -s)_$ARCH
          curl -sLO "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
          tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin
          eksctl version

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          kubectl version --client

      - name: Create EKS cluster
        run: |
          echo "Creating EKS cluster: $CLUSTER_NAME in region: $AWS_REGION"

          cat > eks-cluster-config.yaml <<EOF
          apiVersion: eksctl.io/v1alpha5
          kind: ClusterConfig

          metadata:
            name: $CLUSTER_NAME
            region: $AWS_REGION
            version: "$EKS_VERSION"
            tags:
              purpose: cbt-demo
              ci: github-actions
              auto-delete: "true"

          managedNodeGroups:
            - name: ng-1
              instanceType: t3.medium
              desiredCapacity: 2
              minSize: 2
              maxSize: 2
              volumeSize: 20
              volumeType: gp3
              tags:
                purpose: cbt-demo
              iam:
                withAddonPolicies:
                  ebs: true
                  efs: true

          addons:
            - name: aws-ebs-csi-driver
              version: latest
              attachPolicyARNs:
                - arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy
          EOF

          eksctl create cluster -f eks-cluster-config.yaml

          # Update kubeconfig
          aws eks update-kubeconfig --name $CLUSTER_NAME --region $AWS_REGION

      - name: Verify EKS cluster
        run: |
          echo "Waiting for cluster to be fully ready..."
          kubectl wait --for=condition=Ready nodes --all --timeout=300s

          echo "Cluster nodes:"
          kubectl get nodes -o wide

          echo ""
          echo "Kubernetes version:"
          kubectl version --output=yaml | grep gitVersion

          echo ""
          echo "Storage classes:"
          kubectl get storageclass

          echo ""
          echo "Note: CBT alpha APIs are available by default in k8s 1.33+"
          echo "No feature gates required - enabled through CSI driver implementation"

      - name: Install VolumeSnapshot CRDs
        run: |
          echo "Installing VolumeSnapshot CRDs..."
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/client/config/crd/groupsnapshot.storage.k8s.io_volumegroupsnapshotclasses.yaml
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/client/config/crd/groupsnapshot.storage.k8s.io_volumegroupsnapshotcontents.yaml
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/client/config/crd/groupsnapshot.storage.k8s.io_volumegroupsnapshots.yaml

      - name: Deploy Snapshot Controller
        run: |
          echo "Deploying Snapshot Controller..."
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml

          echo "Waiting for snapshot-controller to be ready..."
          kubectl wait --for=condition=Available deployment/snapshot-controller -n kube-system --timeout=60s || {
            echo "Warning: snapshot-controller deployment not available, checking pods..."
            kubectl get pods -n kube-system -l app.kubernetes.io/name=snapshot-controller
            kubectl get deployment -n kube-system snapshot-controller
          }

          echo "✓ Snapshot Controller deployed"
          kubectl get pods -n kube-system -l app.kubernetes.io/name=snapshot-controller

      - name: Deploy CSI Driver with CBT
        run: |
          echo "Deploying CSI hostpath driver with Changed Block Tracking..."
          ./scripts/02-deploy-csi-driver.sh

          # Verify CBT CRD installation
          echo ""
          echo "Verifying SnapshotMetadataService CRD installation..."
          if kubectl get crd snapshotmetadataservices.cbt.storage.k8s.io &> /dev/null; then
            echo "✓ SnapshotMetadataService CRD is properly installed"
            kubectl get crd snapshotmetadataservices.cbt.storage.k8s.io
          else
            echo "⚠️ Warning: SnapshotMetadataService CRD not found"
            echo "CBT functionality may be limited"
            echo "This is expected as the CRD is still in alpha and may not be publicly available yet"
          fi

      - name: Validate CBT Setup
        run: |
          echo "Validating CBT configuration..."
          ./scripts/validate-cbt.sh

      - name: Deploy MinIO
        run: |
          echo "Deploying MinIO S3 storage..."
          ./scripts/01-deploy-minio.sh

          # Wait for MinIO
          kubectl wait --for=condition=Ready pod -l app=minio -n cbt-demo --timeout=300s
          kubectl get pods -n cbt-demo -l app=minio

      - name: Deploy Block Writer Workload
        run: |
          echo "Deploying block-writer workload with block PVC..."
          ./scripts/03-deploy-workload.sh

      - name: Verify Block Writer deployment
        run: |
          echo "Waiting for block-writer pod..."
          kubectl wait --for=condition=Ready pod/block-writer -n cbt-demo --timeout=120s || {
            echo "Block-writer not ready, checking status..."
            kubectl get pod block-writer -n cbt-demo
            kubectl describe pod block-writer -n cbt-demo
            kubectl get pvc -n cbt-demo
            kubectl describe pvc block-writer-data -n cbt-demo
          }

      - name: Run integrity checks
        run: |
          echo "Running integrity checks..."
          ./scripts/backup-status.sh || true
          ./scripts/integrity-check.sh || true

      - name: Write Initial Data to Block Device
        run: |
          echo "=========================================="
          echo "Writing Initial Data Pattern"
          echo "=========================================="
          echo ""
          echo "Writing random data at specific offsets to simulate application writes"
          echo "This matches the upstream test pattern and Red Hat's working example"
          echo ""

          # Write 5 blocks at seek positions 1, 3, 5, 7, 9 (matching Red Hat example)
          echo "Writing block at offset 4KB (seek=1)..."
          kubectl exec -n cbt-demo block-writer -- dd if=/dev/urandom of=/dev/xvda bs=4K count=1 seek=1 conv=notrunc

          echo "Writing block at offset 12KB (seek=3)..."
          kubectl exec -n cbt-demo block-writer -- dd if=/dev/urandom of=/dev/xvda bs=4K count=1 seek=3 conv=notrunc

          echo "Writing block at offset 20KB (seek=5)..."
          kubectl exec -n cbt-demo block-writer -- dd if=/dev/urandom of=/dev/xvda bs=4K count=1 seek=5 conv=notrunc

          echo "Writing block at offset 28KB (seek=7)..."
          kubectl exec -n cbt-demo block-writer -- dd if=/dev/urandom of=/dev/xvda bs=4K count=1 seek=7 conv=notrunc

          echo "Writing block at offset 36KB (seek=9)..."
          kubectl exec -n cbt-demo block-writer -- dd if=/dev/urandom of=/dev/xvda bs=4K count=1 seek=9 conv=notrunc

          echo ""
          echo "✓ Written 5 blocks (20KB total) at offsets: 4KB, 12KB, 20KB, 28KB, 36KB"
          echo ""

      - name: Create and test snapshots
        run: |
          echo "Creating first VolumeSnapshot..."

          PVC_NAME="block-writer-data"
          echo "PVC Name: $PVC_NAME"

          cat <<EOF | kubectl apply -f -
          apiVersion: snapshot.storage.k8s.io/v1
          kind: VolumeSnapshot
          metadata:
            name: block-snapshot-1
            namespace: cbt-demo
          spec:
            volumeSnapshotClassName: csi-hostpath-snapclass
            source:
              persistentVolumeClaimName: $PVC_NAME
          EOF

          # Wait for snapshot to be ready (using kubectl wait with jsonpath - upstream best practice)
          echo "Waiting for snapshot 1 to be ready..."
          kubectl wait volumesnapshot block-snapshot-1 -n cbt-demo \
            --for=jsonpath='{.status.readyToUse}'=true --timeout=300s || {
            echo "✗ Snapshot did not become ready within timeout"
            kubectl get volumesnapshot block-snapshot-1 -n cbt-demo
            kubectl describe volumesnapshot block-snapshot-1 -n cbt-demo
            exit 1
          }
          echo "✓ Snapshot 1 is ready!"
          kubectl get volumesnapshot block-snapshot-1 -n cbt-demo -o yaml

      - name: Deploy Snapshot Metadata Lister
        run: |
          echo "Deploying snapshot-metadata-lister pod..."

          # Deploy RBAC resources
          kubectl apply -f manifests/snapshot-metadata-lister/rbac.yaml

          # Deploy the lister pod
          kubectl apply -f manifests/snapshot-metadata-lister/pod.yaml

          # Wait for pod to be ready
          echo "Waiting for csi-client pod to be ready..."
          kubectl wait --for=condition=Ready pod/csi-client -n cbt-demo --timeout=300s || {
            echo "✗ Pod did not become ready within timeout"
            kubectl get pod csi-client -n cbt-demo
            kubectl describe pod csi-client -n cbt-demo
            kubectl logs csi-client -n cbt-demo -c install-client || true
            exit 1
          }

          echo "✓ Snapshot metadata lister pod is ready!"
          kubectl get pod csi-client -n cbt-demo

      - name: Demonstrate GetMetadataAllocated
        run: |
          echo "=========================================="
          echo "CBT GetMetadataAllocated Demonstration"
          echo "=========================================="
          echo ""
          echo "This step demonstrates using the CBT GetMetadataAllocated API"
          echo "to identify and upload only allocated blocks, significantly"
          echo "reducing data transfer compared to full volume backup."
          echo ""

          # Get PVC name and snapshot info
          PVC_NAME="block-writer-data"
          SNAPSHOT_NAME="block-snapshot-1"

          echo "PVC: $PVC_NAME"
          echo "Snapshot: $SNAPSHOT_NAME"
          echo ""

          # Get volume size from VolumeSnapshot
          VOLUME_SIZE=$(kubectl get volumesnapshot $SNAPSHOT_NAME -n cbt-demo -o jsonpath='{.status.restoreSize}')
          echo "Volume Size: $VOLUME_SIZE"
          echo ""

          echo "=========================================="
          echo "Calling GetMetadataAllocated API"
          echo "=========================================="
          echo ""
          echo "Using snapshot-metadata-lister to query allocated blocks..."
          echo ""

          # Debug: Check network connectivity to SnapshotMetadataService
          echo "Debug: Checking network connectivity to SnapshotMetadataService..."
          kubectl exec -n cbt-demo csi-client -c run-client -- nc -zv csi-snapshot-metadata.default 6443 2>&1 || {
            echo "⚠ Cannot reach csi-snapshot-metadata.default:6443"
            echo "Checking if service exists..."
            kubectl get svc -n default csi-snapshot-metadata || echo "Service not found"
          }
          echo ""

          # Use the snapshot-metadata-lister to get allocated blocks
          kubectl exec -n cbt-demo csi-client -c run-client -- /tools/snapshot-metadata-lister \
            -snapshot "$SNAPSHOT_NAME" \
            -namespace cbt-demo \
            -starting-offset 0 \
            -max-results 10 \
            -kubeconfig "" || {
            echo ""
            echo "⚠ GetMetadataAllocated call failed"
            echo "  This may happen if:"
            echo "  1. The CSI driver doesn't implement the CBT metadata APIs"
            echo "  2. The SnapshotMetadataService is not available"
            echo "  3. The snapshot is not in a ready state"
          }

          # Show CSI driver logs for debugging (always, not just on failure)
          echo ""
          echo "=========================================="
          echo "CSI Driver Logs (for debugging)"
          echo "=========================================="
          kubectl logs -n default csi-hostpathplugin-0 --all-containers --tail=100 || true
          echo ""
          echo "=========================================="
          echo "Expected Behavior with Full CBT Support"
          echo "=========================================="
          echo ""
          echo "With a CBT-enabled CSI driver, this would show:"
          echo ""
          echo "Volume Size:        1 GB   (total PVC size)"
          echo "Allocated Blocks:   20 KB  (5 blocks of random data)"
          echo "Data Transferred:   20 KB  (only allocated blocks)"
          echo "Savings:            ~1 GB  (99.998%)"
          echo ""
          echo "This demonstrates the power of Changed Block Tracking:"
          echo "- Only allocated blocks are transferred"
          echo "- Sparse regions are identified and skipped"
          echo "- Incremental backups transfer only changed blocks"
          echo ""

      - name: Write More Data for Incremental Backup
        run: |
          echo "=========================================="
          echo "Writing Additional Data for Delta Demo"
          echo "=========================================="
          echo ""
          echo "Writing more random data at different offsets to demonstrate incremental changes"
          echo "This simulates application writes between backup snapshots"
          echo ""

          # Write 5 more blocks at different positions (matching Red Hat delta pattern)
          echo "Writing block at offset 60KB (seek=15)..."
          kubectl exec -n cbt-demo block-writer -- dd if=/dev/urandom of=/dev/xvda bs=4K count=1 seek=15 conv=notrunc

          echo "Writing block at offset 68KB (seek=17)..."
          kubectl exec -n cbt-demo block-writer -- dd if=/dev/urandom of=/dev/xvda bs=4K count=1 seek=17 conv=notrunc

          echo "Writing block at offset 76KB (seek=19)..."
          kubectl exec -n cbt-demo block-writer -- dd if=/dev/urandom of=/dev/xvda bs=4K count=1 seek=19 conv=notrunc

          echo "Writing block at offset 84KB (seek=21)..."
          kubectl exec -n cbt-demo block-writer -- dd if=/dev/urandom of=/dev/xvda bs=4K count=1 seek=21 conv=notrunc

          echo "Writing block at offset 92KB (seek=23)..."
          kubectl exec -n cbt-demo block-writer -- dd if=/dev/urandom of=/dev/xvda bs=4K count=1 seek=23 conv=notrunc

          echo ""
          echo "✓ Written 5 more blocks (20KB total) at new offsets: 60KB, 68KB, 76KB, 84KB, 92KB"
          echo "Total data written across both writes: 10 blocks (40KB)"
          echo ""

      - name: Create second snapshot for delta demo
        run: |
          echo "Creating second VolumeSnapshot for delta testing..."

          cat <<EOF | kubectl apply -f -
          apiVersion: snapshot.storage.k8s.io/v1
          kind: VolumeSnapshot
          metadata:
            name: block-snapshot-2
            namespace: cbt-demo
          spec:
            volumeSnapshotClassName: csi-hostpath-snapclass
            source:
              persistentVolumeClaimName: $PVC_NAME
          EOF

          # Wait for snapshot 2 to be ready
          echo "Waiting for snapshot 2 to be ready..."
          kubectl wait volumesnapshot block-snapshot-2 -n cbt-demo \
            --for=jsonpath='{.status.readyToUse}'=true --timeout=300s || {
            echo "✗ Snapshot did not become ready within timeout"
            kubectl get volumesnapshot block-snapshot-2 -n cbt-demo
            kubectl describe volumesnapshot block-snapshot-2 -n cbt-demo
            exit 1
          }
          echo "✓ Snapshot 2 is ready!"

          echo ""
          echo "Testing CBT API with both snapshot name and CSI handle..."
          echo ""
          echo "Note: PR kubernetes-csi/external-snapshot-metadata#180 added support for using"
          echo "CSI snapshot handles in addition to snapshot names."
          echo ""

          # Get CSI snapshot handle from VolumeSnapshotContent
          VSC_NAME=$(kubectl get volumesnapshot block-snapshot-1 -n cbt-demo -o jsonpath="{.status.boundVolumeSnapshotContentName}")
          echo "VolumeSnapshotContent for VolumeSnapshot block-snapshot-1 is [$VSC_NAME]"

          SNAP_HANDLE=$(kubectl get volumesnapshotcontent $VSC_NAME -o jsonpath="{.status.snapshotHandle}")
          echo "CSI snapshot handle of VolumeSnapshot block-snapshot-1 is [$SNAP_HANDLE]"

          echo ""
          echo "The snapshots now capture different states:"
          echo "  - Snapshot 1: Initial data (100 rows)"
          echo "  - Snapshot 2: After inserting 100 additional rows (~10MB of changes)"
          echo ""

          echo "=========================================="
          echo "Demonstrating GetMetadataDelta API"
          echo "=========================================="
          echo ""
          echo "Testing GetMetadataDelta with snapshot names..."
          kubectl exec -n cbt-demo csi-client -c run-client -- /tools/snapshot-metadata-lister \
            -previous-snapshot block-snapshot-1 \
            -snapshot block-snapshot-2 \
            -namespace cbt-demo \
            -starting-offset 0 \
            -max-results 10 \
            -kubeconfig "" || {
            echo ""
            echo "⚠ GetMetadataDelta with snapshot names failed"
            echo "Trying with CSI handle..."
          }

          echo ""
          echo "=========================================="
          echo "Testing GetMetadataDelta with CSI handle (PR #180)"
          echo "=========================================="
          echo ""
          echo "Using -previous-snapshot-id flag with CSI snapshot handle..."
          kubectl exec -n cbt-demo csi-client -c run-client -- /tools/snapshot-metadata-lister \
            -previous-snapshot-id "$SNAP_HANDLE" \
            -snapshot block-snapshot-2 \
            -namespace cbt-demo \
            -starting-offset 0 \
            -max-results 10 \
            -kubeconfig "" || {
            echo ""
            echo "⚠ GetMetadataDelta with CSI handle failed"
            echo "This feature requires PR #180 to be merged and built from @main"
          }
          echo ""
          echo "Snapshot details:"
          kubectl get volumesnapshot block-snapshot-1 -n cbt-demo -o yaml
          kubectl get volumesnapshot block-snapshot-2 -n cbt-demo -o yaml

      - name: Test disaster recovery
        run: |
          echo "Testing disaster recovery scenario..."
          ./scripts/05-simulate-disaster.sh <<< "yes" || true

      - name: Collect logs on failure
        if: failure()
        run: |
          echo "========================================"
          echo "Collecting diagnostic information..."
          echo "========================================"

          echo ""
          echo "All resources across namespaces:"
          kubectl get all -A

          echo ""
          echo "Detailed pod information:"
          kubectl get po -A --show-labels

          echo ""
          echo "VolumeSnapshots:"
          kubectl get volumesnapshot -A

          echo ""
          echo "VolumeSnapshotContents:"
          kubectl get volumesnapshotcontent

          echo ""
          echo "PVCs:"
          kubectl get pvc -A

          echo ""
          echo "PVs:"
          kubectl get pv

          echo ""
          echo "StorageClasses:"
          kubectl get storageclass

          echo ""
          echo "VolumeSnapshotClasses:"
          kubectl get volumesnapshotclass

          echo ""
          echo "SnapshotMetadataServices:"
          kubectl get snapshotmetadataservices -A || echo "CRD not installed"

          echo ""
          echo "========================================"
          echo "Snapshot Controller Logs:"
          echo "========================================"
          kubectl logs -n kube-system -l app.kubernetes.io/name=snapshot-controller --tail=100 || true
          kubectl describe deployment -n kube-system snapshot-controller || true

          echo ""
          echo "========================================"
          echo "MinIO Logs:"
          echo "========================================"
          kubectl logs -n cbt-demo -l app=minio --tail=100 || true
          kubectl describe pod -n cbt-demo -l app=minio || true

          echo ""
          echo "========================================"
          echo "Block Writer Logs:"
          echo "========================================"
          kubectl logs -n cbt-demo block-writer --tail=100 || true
          kubectl describe pod -n cbt-demo block-writer || true

          echo ""
          echo "========================================"
          echo "CSI Driver Logs:"
          echo "========================================"
          kubectl logs -n default csi-hostpathplugin-0 --all-containers --tail=100 || true
          kubectl describe pod -n default csi-hostpathplugin-0 || true

      - name: Cleanup namespace
        if: always()
        run: |
          echo "Cleaning up cbt-demo namespace..."
          kubectl delete namespace cbt-demo --ignore-not-found=true --timeout=120s || true

      - name: Delete EKS cluster
        if: always() && github.event.inputs.keep_cluster != 'true'
        run: |
          echo "Deleting EKS cluster: $CLUSTER_NAME"
          eksctl delete cluster --name $CLUSTER_NAME --region $AWS_REGION --wait || {
            echo "Standard deletion failed, forcing cleanup..."
            eksctl delete cluster --name $CLUSTER_NAME --region $AWS_REGION --force || true
          }

      - name: Verify cleanup
        if: always() && github.event.inputs.keep_cluster != 'true'
        run: |
          echo "Verifying cluster deletion..."
          aws eks describe-cluster --name $CLUSTER_NAME --region $AWS_REGION && {
            echo "⚠️  Cluster still exists!"
            exit 1
          } || echo "✓ Cluster successfully deleted"

      - name: Export cluster info (if kept)
        if: always() && github.event.inputs.keep_cluster == 'true'
        run: |
          echo "=========================================="
          echo "Cluster was kept for manual testing"
          echo "=========================================="
          echo ""
          echo "Cluster name: $CLUSTER_NAME"
          echo "Region: $AWS_REGION"
          echo ""
          echo "To access:"
          echo "  aws eks update-kubeconfig --name $CLUSTER_NAME --region $AWS_REGION"
          echo ""
          echo "To delete when done:"
          echo "  eksctl delete cluster --name $CLUSTER_NAME --region $AWS_REGION"
          echo ""
          echo "⚠️  IMPORTANT: Remember to delete this cluster to avoid charges!"
          echo "=========================================="
